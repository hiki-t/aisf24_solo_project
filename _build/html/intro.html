
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Welcome to my AI safety project &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intro';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Experiment 1" href="nb/notebook1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="#">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Welcome to my AI safety project
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">My work</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nb/notebook1.html">Exp1</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/notebook2.html">Exp2</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/notebook3.html">Exp3</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fintro.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/intro.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Welcome to my AI safety project</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="welcome-to-my-ai-safety-project">
<h1>Welcome to my AI safety project<a class="headerlink" href="#welcome-to-my-ai-safety-project" title="Link to this heading">#</a></h1>
<br>
<p><em><strong>TL;DR:</strong>
My project replicated the research findings of Anthropic’s Toy Model (Elhage et al., 2022)
and the Sparse Autoencoder (Bricken et al., 2023)
using a single-layer linear model with ReLU filtering,
trained on the MNIST handwritten digit dataset (Exp1-2).
Building on these replications,
I introduced a novel interpretability technique,
which makes the model’s internal representations more transparent
and easier for humans to understand (Exp3).
Codes were provided for each Exp.
You can directly jump into the project summary <a class="reference internal" href="#proj-sum"><span class="xref myst">here</span></a>.</em></p>
<br>
<h2> About my AI safety project </h2>
<br>
<p>During Oct 2024 - Feb 2025,<br />
I participated in the AI Safety Fundamentals (AISF) program, organised by the BlueDot Impact.</p>
<p>I particularly took the Alignment course (<a class="reference external" href="https://aisafetyfundamentals.com/alignment/">https://aisafetyfundamentals.com/alignment/</a>).</p>
<p>This course was composed of two phases: (1) the learning phase, and (2) the project phase.</p>
<p>(1) The learning phase was helpful for building fundamental knowledge in AI Safety (e.g. addressing AI risks and misalignment, RLHF, Scalable oversight, AI model robustness, Mechanistic Interpretability, model risks/capabilities evaluations), developing further research questions and deeply mentally engaging in this field during periods of 8 weeks.</p>
<p>(2) The project phase provided invaluable opportunities to tackle own research questions during periods of 4 weeks (I finished 3 experiments in 3 weeks and 1 week for writing).</p>
<p>My initial goal of this project was to create an interactive project like <a class="reference external" href="https://www.youtube.com/watch?v=1xB2ooTa3yE&amp;amp;t=17s">this youtube clip</a> so that people visiting my project enjoy playing like a mini game, but it was too ambitious to complete. Instead, I made my project simple replications of Anthropic work with toy models and real image dataset with provided codes so that visitors can implement ones by themselves. Also, I maintained running my exps requiring only personal computer power (2024 PC/laptop), not needing a big cloud computing or GPUs so that hopefully people can run my exps locally!</p>
<p>To be noted, my exps are still not perfect because this is still my initial draft. I expect there would be many hassles visitors may encounter while running. I believe, learning something well is through making your own hands dirty! So, I recommend not only reading my project summary, but also running provided codes, experimenting with different settings, and thinking about new research settings and revised code implementations!</p>
<p>This way, I am hoping that new AI safety researchers, engineers, practitioners or curious active visitors can have some hands-on experience on a specific AI safety topic.</p>
<p>Of course, there is no problem to just read my project summary to get a high level gist of my project’s finding and think through future directions or wider applications.</p>
<p>If you can leave some feedback, inspirations, interesting ideas regard to my project, I would love to hear!</p>
<p>My contacts<br />
X/Twitter: <a class="reference external" href="https://x.com/htsujimura">https://x.com/htsujimura</a><br />
Bluesky: <a class="reference external" href="https://bsky.app/profile/htsujimura.bsky.social">https://bsky.app/profile/htsujimura.bsky.social</a><br />
Linkedin: <a class="reference external" href="https://www.linkedin.com/in/hikaru-tsujimura/">https://www.linkedin.com/in/hikaru-tsujimura/</a></p>
<br>
<h2 id="proj-sum"> Project summary </h2>
<br>
<h3 id="intro"> Introduction </h3>
<details>
<summary> <b>For non-technical people, click and read hidden texts here for backgrounds to get familiar with minimum knowledge for the Mechanistic Interpretability method</b> </summary>
<br>
<p>Without a doubt,
the 2020s is the decade we, humans, have started concerning serious threats of AI risks
and AI’s expanding capabilities replacing human labors across a wide range of occupations
(<a class="reference external" href="https://www.linkedin.com/posts/hikaru-tsujimura_ai-aiabrrisk-aiabrthreat-activity-7256778739252916224-RfI3?utm_source=share&amp;amp;utm_medium=member_desktop">see an overview of concerned AI risks on my linkedin post</a>).</p>
<p>Raising more attentions to this concern,
researchers have continuously discovered that
the state of art (SOTA) large language models (LLMs, like the ChatGPT)
or simpler machine learning models/AI systems are
already capable of demonstrating concerning behaviors
(e.g. showing deceiving behaviors to pretend to follow human instructions, hiding true intentions, power acquisition).
If AIs continue to expand their capabilities at this pace with hidden thoughts,
especially secretly holding vicious attitudes towards humanity existence,
humans will be wrecked in the future once super-human AIs are ready for breaking out of human controls.</p>
<p>To mitigate our existential risks
and also make internal minds of advanced AIs more transparent
and predict thier subsequent behaviors,
the Mechanistic Interpretability (Mech Interp),
a ground-breaking novel interpretability approach has been proposed (Olah, et al., 2020).</p>
<p>A primary goal of the Mech Interp method is
to decompose internal complex neural activities of models into
more interpretable components of neural activity patterns or representations
(e.g. ideally interpretable
if one neuron activation frequently corresponds to
a curve-shape in an image, then the neuron is assumed to be a curve-detector neuron).</p>
<p>The minimum interpretable component is called as “features”,
akin to a chemical element in the periodic table (e.g. C or Carbon),
responsible for a simpler distinct information,
such as a particular angle of a curve-shape,
while a collection of features or connections between features is called “circuits”,
akin to a chemical compound, such as CO<sup>2</sup>,
responsible for a more complex information,
such as circular or more complex curve-shape.
The Mech Interp approach has claimed that
these unique neural expressions would coexist
across models to make best use of these neural representations to a wide range of tasks
or circumstances, and this phenamena is called as “universality”.</p>
<p>However, these neural representations are sometimes not simply expressed
because a neuron which is responsible for a curve-shape image
might also be responsible for a dog ear image, which is called “polysemanticity”,
making AI researchers hard to identify
target neurons independently corresponding for single features.</p>
<p>The other research direction has also demonstrated that
a more number of features can be expressed with a less number of neurons,
e.g. expressing four features with two neurons,
by differentiating a combination of two neuronal activations
(e.g. activating
neuron1(10%)-neuron2(90%) for feature1,
neuron1(45%)-neuron2(45%) for feature2,
neuron1(90%)-neuron2(10%) for feature3,
neuron1(0%)-neuron2(0%) for feature4),
whose phenamena is called “superposition” (Elhage et al., 2022).
The Anthropic research team has suggested that
the superposition emerges
when a frequency of representing those features are sparse
(i.e. sparsity level is high) in each neuron
so that there are some rooms for superposition expressions.</p>
<p>To disentangle the polysemanticity and superposition expressions
into the “monosemantic” expressions
– representing only one feature with one corresponding neuron –
the technique of Sparse Autoencoder has been introduced
(Bricken et al., 2023),
enhancing interpretability of models.</p>
</details>
<br>
<p>Although the Anthropic work
(Elhage et al., 2022)
has demonstrated the superposition in toy models,
representing more number of features than a given number of neurons,
they have used synthetic feature data,
which were clean, hypothetically defined
and a small number of features
(i.e. 3~20, although one result demonstrated with 80 features).
In contrast, AI researchers often deal with more noisy,
unknown data (or feature) distribution,
and often failing replication of proposed theories/findings with raw datasets.</p>
<p>Therefore, this project aimed to replicate the Anthropic previous work,
demonstrating a toy model
(i.e. a single-layer linear model with ReLU filtering) of superposition
with the MNIST hand-written digit image database and
investigate further application of the findings with the image dataset.</p>
<p>Specifically, I conducted three experiments (Exp1-3):</p>
<ul class="simple">
<li><p><b>Exp1</b> tested to replicate if superposition would not occur
when data or feature was dense (i.e. sparsity level was 0).</p></li>
<li><p><b>Exp2</b> tested to replicate if superposition would occur
when its sparsity level was high (e.g. 0.999).</p></li>
<li><p>As I successfully replicated Exp1-2, I conducted <b>Exp3</b> to explore further
if features of the real image data can be extracted with
the technique of sparse autoencoder (Bricken et al., 2023),
and test if these features can be converted further
to more human understandable/interpretable information (i.e. image),
by using an image generative model.</p></li>
</ul>
<br>
<h3 id="exp1"> Exp1 </h3>
<p>The Exp1 tested
to replicate one of the Anthropic research outcome (Elhage et al., 2022),
claiming that a toy model of superposition would not be occured
when feature distribution is dense (i.e. sparsity level is zero),
only ending up representing a same number of features with given neurons.
To test this claim,
I implemented a simple linear model
with a single hidden layer
(containing 49 neurons or dimensions)
and ReLU filtering
and trained the model with the MNIST digit dataset
to perform a digit classification task.
After that,
I implemented a classic autoencoder model
to map the 49 feature representations in the linear model
into smaller 2 dim or neurons in the autoencoder model,
inspecting if only two feature representations survive in the hidden layer of the autoencoder model.
See <b>Diagram 1</b> for visual summary of this experiment. For further details of Exp1, check <a class="reference internal" href="nb/notebook1.html"><span class="std std-doc">here</span></a>.</p>
<br>
<p><img alt="alt text" src="_images/exp1_1.png" />
<b>Diagram 1:</b> Visual summary of Exp1. This diagram is mainly for depicting the two model architectures and experiment designs (①-③) used in Exp1.
Please ignore a number of neurons (circles), which are not correct.
① At first, the one-hidden layer linear model with ReLU filtering trained to predict 0-9 digit classes with inputs of 28x28 pixel images.
② After trained the main linear model, an autoencoder model was trained with the trained 49 hidden layer neural activations to reconstruct the original 49 neural activations, bypassing through the smaller hidden layer (2 dim).
③ After trained the autoencoder model, the same digit images were inputed into the main linear model, activating the 49 hidden layer neurons of the linear main model, which in turn activated the 2 hidden layer neurons of the autoencoder model, finally drawing the 49 feature representations in 2 dim.</p>
<br>
<p>With the aforementioned research setting,
I replicated the finding of Anthropic previous work, such that
the 49 features were gradually represented into
the two major feature directions
across training epochs.
When normalized importance* of each feature ranging -1 to 1,
the two important features were directed orthogonally,
and less important features remained relatively close to 0.
Interestingly, some features with large importance directed middle of the two orthogonal directions.
See <b>Figure 1</b> for visual summary of Exp1 result.</p>
<p>*Importance = how much a feature contributes to the MSE loss of the autoencoder model to reconstruct the 49 neural activations of the lienar model. The more contribution, the larger importance value.</p>
<br>
<p><img alt="alt text" src="_images/exp1_2.png" />
<b>Figure 1:</b> Visual summary of Exp1 results.
These line graphs were 49 features/neurons represented in 2 dim of the autoencoder hidden layer across 20 training epochs.
The length of features is a degree of importance, indicating how much a feature contributes to the MSE loss of the autoencoder model to reconstruct the 49 neural activations of the lienar model. The more contribution, the larger importance value.
The horizontal- vertical-coordinates of features were expressed by the 2 neurons of the autoencoder hidden layer, and normalized them into -1 to 1 range.</p>
<br>
<p>Codes, more results and detailed research settings are available <a class="reference internal" href="nb/notebook1.html"><span class="std std-doc">here</span></a>.</p>
<br>
<h3 id="exp2"> Exp2 </h3>
<p>After the first successful replication in Exp1,
showing no superposition of the toy model with dense feature,
Exp2 tested further if supeturerposition would occur
when feature distribution was highly sparse (i.e. sparsity level = 0.999).</p>
<p>In Exp2, experiment design was exactly same as the Exp1,
except a condition
in which feature distribution was maintained sparse
by using sparsity penalty during data transformation
from the 49 dim neural representations of the linear model into
the 2 dim neural representations of the autoencoder model.
See <b>Diagram 2</b> for visual summary of this experiment.
For further details of Exp2, check <a class="reference internal" href="nb/notebook2.html"><span class="std std-doc">here</span></a>.</p>
<br>
<p><img alt="alt text" src="_images/exp2_1.png" />
<b>Diagram 2:</b> Visual summary of Exp1.
Experiment design of Exp2 was exactly same as the Exp1,
except a condition in which
sparsity penalty was applied to the data transformation
from the 49 dim neural representations to the 2 dim neural representations.</p>
<br>
<p>Based on the slight change in the research setting,
I replicated to show superposition of the linear model
with highly sparse features, such that
the 49 features neural representations of the linear model
were represented with
multiple feature directions in the 2 dim autoencoder neurons.
It was unclear what kind of feature representations these were
(e.g. orthogonal, antipodal, pentagon, or more complex geometry),
but it would be interesting to explore future research direction
to investigate feature geometry development with the MNIST image dataset.
See <b>Figure 2</b> for visual summary of Exp2 result.</p>
<br>
<p><img alt="alt text" src="_images/exp2_2.png" />
<b>Figure 2:</b> Visual summary of Exp2 results.
Compared to Exp1 result, here showed superposition of the linear model, representing multiple feature direcitons in the autoencoder model by making features sparse.</p>
<br>
<p>Codes, more results and detailed research settings are available <a class="reference internal" href="nb/notebook2.html"><span class="std std-doc">here</span></a>.</p>
<br>
<h3 id="exp3"> Exp3 </h3>
<p>Building on the successful replications of the Anthropic previous work in Exp1 and Exp2,
demonstrating superpositions of a single-layer linear model with the MNIST digit image dataset,
<b>Exp3</b> explored further
whether superposition of the lienar model with the image datasets
would be applicable for improving interpretability
and transparency of neural representations of the extracted features.</p>
<p>Specifically,
important features identified
in the sparse autoencoder (SAE) model has been used
for “steering”,
a technique to amplifying selective features
compared to other features,
enforcing models to output the steered features more frequently in language models
(e.g. <a class="reference external" href="https://www.anthropic.com/news/mapping-mind-language-model">Anthropic’s Golden Gate Bridge case, 2024</a>).
Extending from the steering technique in the language models,
Exp3 explored
if steering selective features important for each digit
(e.g. feature 1 for digit 9) in the SAE model
can reconstruct digit-like (e.g. 9-like) images,
transparently interpreting that
selective features are crucial for
representing an digit-like image
or a particular part
of the digit-like image
(e.g. circular part, ending or outline of 9).
See <b>Diagram 3</b> for visual summary of this experiment.
For further details of Exp3, check <a class="reference internal" href="nb/notebook3.html"><span class="std std-doc">here</span></a>.</p>
<br>
<p><img alt="alt text" src="_images/exp3_1.png" />
<b>Diagram 3:</b> Visual summary of Exp3. This diagram is mainly for depicting the three model architectures and experiment designs (①-➅) used in Exp3.
Please ignore a number of neurons (circles) on the graph, which are not correct.
①-② followed the same procedures as Exp1 and 2, except that the sparse autoencoder model has 98 dim neurons in its hidden layer. Briefly, the one-layer linear model trained with the MNIST image datasets and the Sparse autoencoder (SAE) model trained with the trained 49 hidden layer neural activations of the linear model to reconstruct the original 49 neural activations, bypassing through the larger hidden layer (98 dim).
③-➃ An image generative model (or a decoder) trained with the trained 49 hidden layer neural activations of the linear model to reconstruct the original 784 dim of flattened images.
➄-➅ After trained the SAE model, only activating selective n/98 hidden layer neurons (e.g. feature1 important for digit 9) of the SAE model and set other neuron activation to 0, which in turn reconstructing the 49 neural activations of the linear model, which in turn reconstructing 784 dim of flattened images by the image Generative model.</p>
<br>
<p>Implementing an explorative interpretability approach,
using a combination of the linear model (the main model for the digit classification task with the MNIST image datasets),
the sparse autoencoder model (for representing relatively monosemantic-like features than the main model),
and the image generative model (for reconstructing the image datasets the from selective features important for each digit class)
preliminarily collected interesting results.
As expected, the feat histogram
(histograms of activations level of features for each digit class)
demonstrated that
only a few features (2~5/98) of the SAE model strongly contributed to the MSE loss for each digit class
to reconstruct the 49 dim hidden layer of the linear model.
Additionally,
only activating the selective key features
(and deactivating other features setting to 0)
led to reconstruct each digit-like images.
These features-based digit image reconstructions were unsupervised (not specifically trained),
and purely achieved by transparently interpreting key features with the human-understandable approach.
See <b>Figure 3</b> for visual summary of Exp3 result.</p>
<br>
<p><img alt="alt text" src="_images/exp3_2.png" />
<b>Figure 3:</b> Visual summary of Exp3 results.
The left column showed a collection of histograms of 98 feature activations with two border lines (orange dashed line for the 90th percentile and black dashed line for expected probability = 1/98). The activation level was normalized so that a sum of all activation level was 1. This histogram showed the results when the SAE model sparsity level = 0.999.
The middle column provided an example visual aids demonstrating how each digit class activated selective neurons/features important for each digit class (and deactivated non-important features), which reconstructed the 49 dim hidden layer of the linear model.
The right column drew how the selective features important for each digit class ended up reconstructing each digit class-like 28x28 image with the image generative model.</p>
<br>
<p>Codes, more results and detailed research settings are available <a class="reference internal" href="nb/notebook3.html"><span class="std std-doc">here</span></a>.</p>
<br>
<h3 id="exp3"> Final conclusion and future direction </h3>
<p>Exp1-2 replicated to demonstrate
superposition of the toy models (a single layer linear model with ReLU filtering)
with the MNIST hand-written digit image datasets.
This supports that even a simple linear model represents
superposition with raw datasets of image domain.
This suggests that
it would be worthwhile to explore further
whether more scaled models or datasets or more complex image data
(e.g. more diverse range of images) would also represent superpositon.</p>
<p>The other novel findings in Exp3 showed that
more human understandable interpretability technique could be developed
by using a combination of
a main model (conducting a main task),
a SAE model (extracting relatively more mono-semantic-like features than the main model), and
a generative model (reconstructing inputs/datasets from hidden layer(s) of the main model),
and reconstructing inputs/datasets-like stimuli by activating key features.
It would be useful if this combined technique could be applicable to a variety of tasks, domains and goals
by switching each of the aforementioned three roles with the most relevant ones.</p>
<br>
<h3 id="ref"> References </h3>
<ul class="simple">
<li><p>Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., … &amp; Olah, C. (2022). Toy models of superposition. arXiv preprint arXiv:2209.10652.</p></li>
<li><p>Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., … &amp; Olah, C. (2023). Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2.</p></li>
<li><p>Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., … &amp; Henighan, T. (2024). Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread.</p></li>
</ul>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="right-next"
       href="nb/notebook1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Experiment 1</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>