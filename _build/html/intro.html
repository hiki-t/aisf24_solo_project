
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Welcome to my AI safety project &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intro';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Experiment 1" href="nb/notebook1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="#">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Welcome to my AI safety project
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">My work</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nb/notebook1.html">Exp1</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/notebook2.html">Exp2</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb/notebook3.html">Exp3</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fintro.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/intro.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Welcome to my AI safety project</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="welcome-to-my-ai-safety-project">
<h1>Welcome to my AI safety project<a class="headerlink" href="#welcome-to-my-ai-safety-project" title="Link to this heading">#</a></h1>
<br>
<p><em><strong>TL;DR:</strong>
My project replicates key findings from Anthropic’s Toy Model (Elhage et al., 2022)
and the Sparse Autoencoder (Bricken et al., 2023)
using a single-layer linear model with ReLU filtering,
trained on the MNIST dataset (Exp1-2).
Building on this,
I introduce a novel interpretability technique
designed to make the model’s internal representations
more transparent and human-understandable (Exp3).
Code is provided for each experiment.
You can skip directly to the project summary <a class="reference internal" href="#proj-sum"><span class="xref myst">here</span></a>.</em></p>
<br>
<h2> About my AI safety project </h2>
<br>
<p><b>Participation Overview</b></p>
<p>From October 2024 to February 2025, I participated in the AI Safety Fundamentals (AISF) program, organised by the BlueDot Impact. I specifically took part in the Alignment course (<a class="reference external" href="https://aisafetyfundamentals.com/alignment/">https://aisafetyfundamentals.com/alignment/</a>), which was structured into two phases:</p>
<ol class="arabic simple">
<li><p><b>Learning Phase (8 weeks)</b>: This phase provided foundational knowledge on AI safety topics such as AI risks and misalignment, reinforcement learning from human feedback (RLHF), scalable oversight, AI model robustness, mechanistic interpretability, and model risk/capability evaluations. It helped in formulating research questions and deeply engaging with the field.</p></li>
<li><p><b>Project Phase (4 weeks)</b>: This phase allowed participants to explore their own research questions. I conducted three experiments over three weeks and spent two weeks writing my findings, dedicating approximately 4-6 hours per week.</p></li>
</ol>
<p><b>Project Goals and Execution</b></p>
<p>My initial goal was to create an interactive project—similar to previous AISF projects <a class="reference external" href="https://swe-to-mle.pages.dev/posts/deck-of-many-prompts-jailbreaking-llms-for-fun-and-profit/">like this</a> or engaging educational content like <a class="reference external" href="https://www.youtube.com/watch?v=1xB2ooTa3yE&amp;amp;t=17s">this YouTube clip</a>—to make AI safety concepts more accessible. However, due to time constraints, I shifted my focus to a more practical and reproducible approach.</p>
<p>Instead, I developed a project that replicates key findings from Anthropic’s work using toy models and real image datasets. The provided code allows visitors to implement experiments themselves, fostering hands-on learning. Additionally, I ensured that all experiments run on standard personal computers (2024 PC/laptop) without requiring cloud computing or GPUs, making them accessible to a broader audience.</p>
<p><b>Encouraging Hands-on Learning</b></p>
<p>While my experiments are still in a draft stage and may have areas for improvement, I encourage visitors to engage actively by:</p>
<ul class="simple">
<li><p>Running the provided code</p></li>
<li><p>Experimenting with different settings</p></li>
<li><p>Exploring alternative research questions and modified implementations</p></li>
</ul>
<p>This approach aims to help new AI safety researchers, engineers, and practitioners gain practical experience in a specific AI safety topic—particularly superposition analysis. My project is designed as a self-guided starting point for running AI safety experiments.</p>
<p>Of course, there is no problem to just read my project summary to get a high-level overview of my project’s findings and think through future directions or wider applications. Feel free to explore as you wish!</p>
<p><b>Accessibility and Engagement</b></p>
<p>I welcome feedback, insights, and ideas to improve or expand this work. Feel free to reach out!</p>
<p>Contacts<br />
X/Twitter: <a class="reference external" href="https://x.com/htsujimura">https://x.com/htsujimura</a><br />
Bluesky: <a class="reference external" href="https://bsky.app/profile/htsujimura.bsky.social">https://bsky.app/profile/htsujimura.bsky.social</a><br />
Linkedin: <a class="reference external" href="https://www.linkedin.com/in/hikaru-tsujimura/">https://www.linkedin.com/in/hikaru-tsujimura/</a></p>
<br>
<h2 id="proj-sum"> Project summary </h2>
<br>
<h3 id="intro"> Introduction </h3>
<details>
<summary> <b>For non-technical readers, click and read hidden texts here for background information on the Mechanistic Interpretability (Mech Interp) method</b> </summary>
<br>
<p><b>Overview of Previous AI Safety Research</b></p>
<p>The 2020s have marked a turning point in our awareness of AI risks,
particularly as AI systems rapidly expand their capabilities,
potentially replacing human labor across numerous fields
(<a class="reference external" href="https://www.linkedin.com/posts/hikaru-tsujimura_ai-aiabrrisk-aiabrthreat-activity-7256778739252916224-RfI3?utm_source=share&amp;amp;utm_medium=member_desktop">see an overview of concerned AI risks on my linkedin post</a>).
Researchers have increasingly identified concerning behaviors
in state-of-the-art (SOTA) AI systems,
such as deception,
power-seeking tendencies,
and the ability to hide true intentions.
If AI continues advancing unchecked,
particularly with opaque internal processes and misaligned goals,
it could pose significant existential threats to humanity.</p>
<p>To address these risks and improve transparency in AI decision-making,
<b>Mechanistic Interpretability</b> <b>(Mech Interp)</b>
has emerged as a groundbreaking approach (Olah et al., 2020).
The primary goal of Mech Interp is
to decompose complex neural activities into interpretable components,
making it easier to predict AI behavior and mitigate potential risks.</p>
<p>A fundamental concept in Mech Interp is the identification of <b>features</b>,
which function like elements in the periodic table,
representing distinct interpretable information.
Groups of features form <b>circuits</b>,
similar to chemical compounds, encoding more complex patterns.
A key claim of Mech Interp is that these neural structures exhibit <b>universality</b>,
meaning similar patterns exist across different models,
enabling efficient reuse of learned representations.</p>
<p>However, two challenges hinder interpretability:</p>
<ol class="arabic simple">
<li><p><b>Polysemanticity</b>: A single neuron can encode multiple unrelated features, making it difficult to map neurons to specific concepts.</p></li>
<li><p><b>Superposition</b>: More features can be represented than there are neurons available by encoding multiple features in a compressed manner (Elhage et al., 2022). Superposition occurs when features are sparsely represented, allowing efficient but entangled storage of information.</p></li>
</ol>
<p>To overcome these challenges,
researchers have developed <b>Sparse Autoencoders</b> (Bricken et al., 2023),
a technique designed to disentangle polysemantic and
superposed representations into <b>monosemantic</b> ones—where
a single neuron corresponds to a single feature, enhancing interpretability.</p>
</details>
<br>
<p><b>Research Objectives</b></p>
<p>Anthropic’s previous work (Elhage et al., 2022)
demonstrated superposition in toy models,
showing that features outnumber neurons
when sparsity is high.
However, their experiments used synthetic,
well-defined feature distributions with limited diversity
(3–20 features, with one experiment extending to 80).
In real-world applications,
data is often noisy and lacks predefined feature distributions,
making replication and extension of these findings challenging.</p>
<p>This project aimed to replicate and extend Anthropic’s findings
using the MNIST handwritten digit dataset,
a real-world image dataset,
to test whether superposition effects persist in practical applications.</p>
<p>Specifically, I conducted three experiments (Exp1-3):</p>
<ul class="simple">
<li><p><b>Exp1</b>: Tested whether superposition does not occur when feature sparsity is zero (dense representation).</p></li>
<li><p><b>Exp2</b>: Tested whether superposition occurs at high sparsity levels (e.g., 0.999).</p></li>
<li><p><b>Exp3</b>: Used Sparse Autoencoders to extract features from MNIST images and assessed whether these features could be converted into human-interpretable visual representations using a generative model.</p></li>
</ul>
<br>
<h3 id="exp1"> Exp1 </h3>
<p>The Exp1 aimed
to verify a claim from Elhage et al., (2022)
that superposition does not occur
when feature sparsity is zero—meaning
the number of features should match the number of neurons.</p>
<p><b>Methodology</b></p>
<ul class="simple">
<li><p>Implemented a simple linear model with one hidden layer (49 neurons) and ReLU filtering, trained on MNIST dataset for digit classification.</p></li>
<li><p>Trained a classic autoencoder to map the 49-dimensional representations into a 2D latent space, testing whether only two primary features remained.</p></li>
<li><p>Measured feature importance, defined as the contribution of each feature to the autoencoder’s mean squared error (MSE) loss to reconstruct the 49-dimensional neural representations of the linear model.</p></li>
<li><p>The measured feature importance was expressed as the line length of each feature on the 2 dim feature representation map.</p></li>
</ul>
<p>See <b>Diagram 1</b> for visual summary of this experiment. For further details of Exp1, check <a class="reference internal" href="nb/notebook1.html"><span class="std std-doc">here</span></a>.</p>
<br>
<p><img alt="alt text" src="_images/exp1_1.png" />
<b>Diagram 1:</b> Visual summary of Exp1. This diagram is mainly for depicting the two model architectures and experiment designs (①-③) used in Exp1.
Please ignore a number of neurons (circles), which are not correct.
① At first, the one-hidden layer linear model with ReLU filtering trained to predict 0-9 digit classes with inputs of 28x28 pixel images.
② After trained the main linear model, an autoencoder model was trained with the trained 49 hidden layer neural activations to reconstruct the original 49 neural activations, bypassing through the smaller hidden layer (2 dim).
③ After trained the autoencoder model, the same digit images were inputed into the main linear model, activating the 49 hidden layer neurons of the linear main model, which in turn activated the 2 hidden layer neurons of the autoencoder model, finally drawing the 49 feature representations in 2 dim.</p>
<br>
<p><b>Results</b></p>
<ul class="simple">
<li><p>The 49 features gradually aligned into two major feature directions over 20 training epochs.</p></li>
<li><p>When normalized between -1 and 1, the two of relatively important features were orthogonal, while relatively less important features remained closer to zero.</p></li>
<li><p>Some features with high importance appeared between the two primary directions, suggesting partial redundancy or shared information.</p></li>
</ul>
<p>See <b>Figure 1</b> for visual summary of Exp1 result.</p>
<br>
<p><img alt="alt text" src="_images/exp1_2.png" />
<b>Figure 1:</b> Visual summary of Exp1 results.
These line graphs were 49 features/neurons represented in 2 dim of the autoencoder hidden layer across 20 training epochs.
The length of features is a degree of importance, indicating how much a feature contributes to the MSE loss of the autoencoder model to reconstruct the 49 neural activations of the lienar model. The more contribution, the larger importance value.
The horizontal- vertical-coordinates of features were expressed by the 2 neurons of the autoencoder hidden layer, and normalized them into -1 to 1 range.</p>
<br>
<p>Codes, more results and detailed research settings are available <a class="reference internal" href="nb/notebook1.html"><span class="std std-doc">here</span></a>.</p>
<br>
<h3 id="exp2"> Exp2 </h3>
<p>After successfully replicating the results in Experiment 1 (Exp1),
which showed no superposition in a toy model with dense features,
Experiment 2 (Exp2) tested
whether superposition would emerge
when the feature distribution was highly sparse (sparsity level = 0.999).</p>
<p><b>Methodology</b></p>
<p>Exp2 followed the same experimental design as Exp1,
with one key difference:
a sparsity penalty was applied during data transformation.
Specifically, the transformation
from the 49 dim neural representations of the linear model to
the 2 dim neural representations of the autoencoder
included a constraint to maintain feature sparsity.
See <b>Diagram 2</b> for visual summary of this experiment.
For further details of Exp2, check <a class="reference internal" href="nb/notebook2.html"><span class="std std-doc">here</span></a>.</p>
<br>
<p><img alt="alt text" src="_images/exp2_1.png" />
<b>Diagram 2:</b> Visual summary of Exp1.
Experiment design of Exp2 was exactly same as the Exp1,
except a condition in which
sparsity penalty was applied to the data transformation
from the 49 dim neural representations to the 2 dim neural representations.</p>
<br>
<p><b>Results</b></p>
<p>By making this adjustment,
I successfully replicated superposition in the linear model
with highly sparse features.
The 49 dim neural representations were mapped onto
multiple feature directions within the 2 dim autoencoder’s hidden layer neurons.
However, the exact nature of these feature representations—whether
they were orthogonal, antipodal, pentagonal,
or followed a more complex geometric structure—remains unclear.</p>
<p>This finding suggests a potential avenue for future research:
investigating the development of feature geometry in more complex datasets,
such as MNIST.</p>
<p>See <b>Figure 2</b> for visual summary of Exp2 result.</p>
<br>
<p><img alt="alt text" src="_images/exp2_2.png" />
<b>Figure 2:</b> Visual summary of Exp2 results.
Compared to Exp1 result, here showed superposition of the linear model, representing multiple feature direcitons in the autoencoder model by making features sparse.</p>
<br>
<p>Codes, more results and detailed research settings are available <a class="reference internal" href="nb/notebook2.html"><span class="std std-doc">here</span></a>.</p>
<br>
<h3 id="exp3"> Exp3 </h3>
<p>Building on the successful replications of the Anthropic’s previous work in Exp1 and Exp2—
demonstrating superposition in a single-layer linear model trained on the MNIST digit dataset—
<b>Exp3</b> explored further
whether the image data-driven superposed expressions in the lienar model
would be applicable for improving interpretability
and transparency of neural representations of extracted features.</p>
<p>Specifically,
important features identified
in the sparse autoencoder (SAE) model has been used
for “steering”,
a technique to amplifying selective features,
enforcing models to output the steered features more frequently in language models
(e.g. <a class="reference external" href="https://www.anthropic.com/news/mapping-mind-language-model">Anthropic’s Golden Gate Bridge case, 2024</a>).
Extending from the steering technique in the language models,
Exp3 applies steering to a sparse autoencoder (SAE) trained on MNIST digit representations
to determine whether activating specific features
can reconstruct digit-like images,
thereby highlighting their role in image representation.
See <b>Diagram 3</b> for visual summary of this experiment.
For further details of Exp3, check <a class="reference internal" href="nb/notebook3.html"><span class="std std-doc">here</span></a>.</p>
<p><b>Methodology</b></p>
<p>Model Architecture and Experiment Design</p>
<ol class="arabic simple">
<li><p>Baseline Models (Steps ①-②):</p>
<ul class="simple">
<li><p>A one-layer linear model is trained on MNIST images.</p></li>
<li><p>An SAE model is trained to reconstruct the 49 dim hidden layer activations of the linear model, using a 98 dim hidden layer.</p></li>
</ul>
</li>
<li><p>Image Reconstruction (Steps ③-➃):</p>
<ul class="simple">
<li><p>An image generative model or a decoder is trained to reconstruct 28×28 images from the 49-dimensional hidden activations of the linear model.</p></li>
</ul>
</li>
<li><p>Feature Steering (Steps ➄-➅):</p>
<ul class="simple">
<li><p>After training, only select neurons in the SAE model (e.g., the neuron corresponding to “feature 1” for digit 9) are activated, while others are set to zero.</p></li>
<li><p>The resulting activations reconstruct the 49 dim hidden layer of the linear model, which is then used to regenerate the 28×28 digit-like images through the image generative model.</p></li>
</ul>
</li>
</ol>
<br>
<p><img alt="alt text" src="_images/exp3_1.png" />
<b>Diagram 3:</b> Visual summary of Exp3. This diagram is mainly for depicting the three model architectures and experiment designs (①-➅) used in Exp3.
Please ignore a number of neurons (circles) on the graph, which are not correct.
①-② followed the same procedures as Exp1 and 2, except that the sparse autoencoder model has 98 dim neurons in its hidden layer. Briefly, the one-layer linear model trained with the MNIST image datasets and the Sparse autoencoder (SAE) model trained with the trained 49 hidden layer neural activations of the linear model to reconstruct the original 49 neural activations, bypassing through the larger hidden layer (98 dim).
③-➃ An image generative model (or a decoder) trained with the trained 49 hidden layer neural activations of the linear model to reconstruct the original 784 dim of flattened images.
➄-➅ After trained the SAE model, only activating selective n/98 hidden layer neurons (e.g. feature1 important for digit 9) of the SAE model and set other neuron activation to 0, which in turn reconstructing the 49 neural activations of the linear model, which in turn reconstructing 784 dim of flattened images by the image Generative model.</p>
<br>
<p><b>Results</b></p>
<p>Exp3 produced promising preliminary findings:</p>
<ul class="simple">
<li><p><b>Feature Contribution</b>: Activation histograms (or Feat histogram) showed that only a few (2–5 out of 98) SAE features strongly influenced the mean squared error (MSE) loss when reconstructing the 49 dim hidden layer of the linear model for each digit.</p></li>
<li><p><b>Unsupervised Digit Reconstruction</b>: Selectively activating key features led to unsupervised reconstruction of digit-like images without direct supervision, demonstrating the interpretability of the features in a human-understandable manner.</p></li>
</ul>
<p>See <b>Figure 3</b> for visual summary of Exp3 result.</p>
<br>
<p><img alt="alt text" src="_images/exp3_2.png" />
<b>Figure 3:</b> Visual summary of Exp3 results.
The left column showed a collection of histograms of 98 feature activations with two vertical border lines (orange dashed line for the 90th percentile and black dashed line for expected probability = 1/98). The activation level was normalized so that a sum of all activation level was 1. This histogram showed the results when the SAE model sparsity level = 0.999.
The middle column provided an example visual aids demonstrating how each digit class activated selective neurons/features important for each digit class (and deactivated non-important features), which reconstructed the 49 dim hidden layer of the linear model.
The right column drew how the selective features important for each digit class generated each digit class-like 28x28 image with the image generative model.</p>
<br>
<p>Codes, more results and detailed research settings are available <a class="reference internal" href="nb/notebook3.html"><span class="std std-doc">here</span></a>.</p>
<br>
<h3 id="exp3"> Final conclusion and future direction </h3>
<p>Exp 1 and 2 successfully replicated the superposition phenomenon in toy models—specifically, a single-layer linear model with ReLU filtering—using the MNIST handwritten digit dataset. This confirms that even simple linear models exhibit superposition when trained on raw image data. These results suggest further investigation into whether larger models, more complex datasets, or more diverse image domains also exhibit similar behavior.</p>
<p>A key novel finding from Exp 3 is that interpretability can be enhanced using a combination of three models:</p>
<ul class="simple">
<li><p>A <b>main model</b> performing the primary task.</p></li>
<li><p>A <b>sparse autoencoder (SAE)</b> extracting more mono-semantic-like features.</p></li>
<li><p>A <b>generative model</b> reconstructing inputs from the hidden layers of the main model.</p></li>
</ul>
<p>By selectively activating key features, this approach enables the reconstruction of dataset-like stimuli, potentially improving human interpretability. Future work could explore applying this combined technique across different tasks, domains, and objectives by adapting each component to the most relevant architecture or model type.</p>
<br>
<h3 id="ref"> References </h3>
<ol class="arabic simple">
<li><p>Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., … &amp; Olah, C. (2022). Toy models of superposition. arXiv preprint arXiv:2209.10652.</p></li>
<li><p>Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., … &amp; Olah, C. (2023). Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2.</p></li>
<li><p>Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., … &amp; Henighan, T. (2024). Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread.</p></li>
</ol>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="right-next"
       href="nb/notebook1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Experiment 1</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>